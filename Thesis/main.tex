
\documentclass[twoside,12pt, a4paper]{report}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\setlength {\marginparwidth }{2.5cm}
\usepackage[left=3cm,right=3cm,top=2.5cm]{geometry}
\usepackage{float}
\usepackage{titlesec}
\usepackage{forest}
\usepackage{listings}
\usepackage{color}
\usepackage{eurosym}
\usepackage{ftnxtra}
\usepackage[fencedCode]{markdown}
\usepackage[T1]{fontenc}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{minted}
\usepackage{fnpos}
\usepackage[ruled,vlined]{algorithm2e}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage{hyperref}
\hypersetup{linktocpage}

\usepackage{booktabs} % Pour de meilleurs tableaux

\usepackage{longtable}
\providecommand{\keywords}[1]
{
  
  \textbf{Keywords---} #1
}

\urlstyle{same}


\setcounter{secnumdepth}{4}

%\usepackage{biblatex} %Imports biblatex package
\usepackage[
    backend=biber,
    style=numeric,
    sortlocale=fr_FR,
    natbib=true,
    url=false,
    doi=true,
    eprint=false
]{biblatex}
\addbibresource{references.bib} %Import the bibliography file



\definecolor{darkWhite}{rgb}{0.94,0.94,0.94}
 
\lstset{
  aboveskip=3mm,
  belowskip=-2mm,
  backgroundcolor=\color{darkWhite},
  basicstyle=\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{red},
  deletekeywords={...},
  escapeinside={\%*}{*)},
  extendedchars=true,
  framexleftmargin=16pt,
  framextopmargin=3pt,
  framexbottommargin=6pt,
  frame=tb,
  keepspaces=true,
  keywordstyle=\color{blue},
  language=C,
  literate=
  {²}{{\textsuperscriptS}}1
  {⁴}{{\textsuperscript{4}}}1
  {⁶}{{\textsuperscript{6}}}1
  {⁸}{{\textsuperscript{8}}}1
  {€}{{\euro{}}}1
  {é}{{\'e}}1
  {è}{{\`{e}}}1
  {ê}{{\^{e}}}1
  {ë}{{\¨{e}}}1
  {É}{{\'{E}}}1
  {Ê}{{\^{E}}}1
  {û}{{\^{u}}}1
  {ù}{{\`{u}}}1
  {â}{{\^{a}}}1
  {à}{{\`{a}}}1
  {á}{{\'{a}}}1
  {ã}{{\~{a}}}1
  {Á}{{\'{A}}}1
  {Â}{{\^{A}}}1
  {Ã}{{\~{A}}}1
  {ç}{{\c{c}}}1
  {Ç}{{\c{C}}}1
  {õ}{{\~{o}}}1
  {ó}{{\'{o}}}1
  {ô}{{\^{o}}}1
  {Õ}{{\~{O}}}1
  {Ó}{{\'{O}}}1
  {Ô}{{\^{O}}}1
  {î}{{\^{i}}}1
  {Î}{{\^{I}}}1
  {í}{{\'{i}}}1
  {Í}{{\~{Í}}}1,
  morekeywords={*,...},
  numbers=left,
  numbersep=10pt,
  numberstyle=\tiny\color{black},
  rulecolor=\color{black},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stepnumber=1,
  stringstyle=\color{gray},
  tabsize=4,
  title=\lstname,
}

\lstdefinestyle{frameStyle}{
    basicstyle=\footnotesize,
    numbers=left,
    numbersep=20pt,
    numberstyle=\tiny\color{black}
}





\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\begin{document}
\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Université Libre de Bruxelles}\\[1.5cm] % Name of your university/college
\textsc{\Large }\\[0.5cm] % Major heading such as course name

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge D \bfseries }\\[0.4cm] % Title of your document
\HRule \\[1.2cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Authors:}\\
Soufiane \textsc{AJOUAOU}\\  000459811\\


\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.5\textwidth}
\begin{flushright} 
\emph{Supervisors:} \\
 \textsc{}\\


\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[width=3.3cm, height=3.3cm]{sceau-a-quadri.jpg}%% Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\end{titlepage}


\section*{Abstract}

BLABLABLA
\vspace{10pt}

%TC:ignore
\keywords{AAA, BBB, DDD, CCCC, TTTT}
\newpage

\section*{Acknowledgements}
BLABLABLA
\newpage


\tableofcontents

\listoffigures
\listoftables
\lstlistoflistings

\newpage

\chapter{Introduction}
In the domain of cartography, the challenge of simplifying complex geographical data for clear and effective representation has grown increasingly important. As our reliance on automated cartography powered by computer technologies continues to expand, the demand for efficient algorithms becomes ever more critical, particularly in the realms of feature extraction and simplification. Among these algorithms, the Douglas-Peucker line simplification method has remained a foundational tool for representing linear features.

This research is dedicated to extending the frontiers of cartographic representation, with a primary focus on POSTGIS and MobilityDB \cite{zimanyi2019mobilitydb}, by transforming the Douglas-Peucker algorithm into a streaming paradigm. Unlike the traditional application of Douglas-Peucker on static datasets, a streaming algorithm processes data dynamically and continuously, making it ideally suited for real-time or rapidly evolving input. This dynamic adaptability is crucial in modern applications such as GPS navigation, real-time geospatial data analysis, and the constant monitoring of geospatial data streams, all of which are integral to the POSTGIS and MobilityDB ecosystem.

In this study, our paramount objective is the simplification of line representations of mobility data within the POSTGIS and MobilityDB environment. This entails processing an ordered set of n+1 points in a plane, forming a polygonal path comprised of line segments, and deriving a streamlined path with fewer segments while retaining the essential characteristics of the original path. The key distinction here is that our research focuses on the streaming nature of the data, aligning seamlessly with the principles and capabilities of POSTGIS and MobilityDB.

An underlying assumption for this work is the simplicity of the provided path, with an absence of self-intersections. The presence of self-intersections in cartographic data often indicates issues related to digitization errors. While we aspire to maintain the simplicity of the resultant approximation, the question of computational feasibility within the POSTGIS and MobilityDB environment remains a central theme.

The concept of "representing well" encompasses a variety of dimensions, including maintaining proximity between the original and simplified paths, minimizing the area between them, integrating critical points from the original path into the simplified version, and optimizing other measures of curve discrepancy. This study builds upon the established foundations of the Douglas-Peucker algorithm and pioneers the integration of stream data processing into the POSTGIS and MobilityDB framework. This innovation opens new horizons in real-time and dynamic cartographic applications, with a key focus on mobility data.

In summary, this thesis embarks on a pioneering journey to generalize the Douglas-Peucker algorithm for streaming line simplification, with a primary emphasis on the POSTGIS and MobilityDB ecosystem. The motivation behind this endeavor is rooted in the ever-growing demand for adapting cartographic methods to real-time data streams, particularly in the context of dynamic geospatial mobility data.

\iffalse
\subsection{Definitions and Key Concepts}
In this subsection, we will establish a foundational understanding of key terms and concepts central to the research presented in this thesis. These definitions are essential for comprehending the context and significance of the study.

\subsubsection{Moving Objects} 

Moving objects refer to entities or points that continuously change their positions in a given space over time. In the context of this research, moving objects represent dynamic or mobile assets, including vehicles, individuals, or any entities with ever-changing spatial coordinates. Understanding the behavior of moving objects is fundamental for the stream algorithm proposed in this study.

\subsubsection{Stream Data} 

Stream data pertains to the continuous and real-time flow of data that arrives sequentially. In the context of this research, stream data represents the dynamic nature of the information being processed. Stream data includes data points, such as the changing positions of moving objects, which are received and processed as they become available over time.


\subsubsection{Polygonal Path} 

A polygonal path is a sequence of line segments connected end-to-end, forming a coherent trajectory or route. In the context of this research, a polygonal path represents the course taken by moving objects, represented as a series of line segments, which is the primary focus of the stream algorithm for simplification.


\subsubsection{Restricted Version} 

A restricted version in this context signifies a specialized or limited form of an algorithm or problem. It is tailored to accommodate specific constraints or requirements pertinent to the application. For example, a restricted version of a line simplification algorithm may be designed to address particular limitations associated with moving objects and streaming data.



\subsubsection{Function Error} 

Function error indicates the degree of discrepancy between an ideal or reference function and an approximation or simplified representation. In the context of line simplification, it quantifies the degree to which the simplified line retains the essential characteristics of the original polygonal path.

\subsubsection{Fréchet Distance} 

The Fréchet distance is a distance metric that quantifies the similarity between two curves or paths. It measures the likeness between the original polygonal path and a simplified path by considering the minimum separation distance between a moving object on each path as they traverse from start to end.


\subsubsection{The Hausdorff Distance} 

The Hausdorff distance is another distance metric used to measure the similarity or dissimilarity between two sets or paths. It calculates the maximum distance from each point in one set to the nearest point in the other set, providing a measure of the dissimilarity between paths.


\subsubsection{Convex Paths} 

Convex paths represent polygonal paths with the property that any line segment connecting two points lies entirely within the convex hull of the points. Convex paths exhibit certain geometric properties that can be advantageous in the context of simplification algorithms.

\subsubsection{XY-Monotone Paths} 

An XY-monotone path is a polygonal path with the characteristic of being monotonic in both the x and y directions. Such paths move in one direction along the x and y axes without backtracking.

\subsubsection{General Paths} 

General paths encompass polygonal paths without specific constraints on their geometry. They may exhibit complex shapes, including non-monotonic behavior, sharp turns, and diverse geometric characteristics.

\fi

\chapter{State of the Art}
In this section, we will provide a literature review on line simplification algorithms, stream processing algorithms and subjects related to the topic of this thesis.

\section{Error Metrics}

In this section we will explain how to compare a trajectory with its simple counterpart in detail. In this study, a trajectory is represented as a polyline P, which is made up of a series of points  $\{p_{1},..., p_{n}\}$, where the points pi are made up of the longitude, latitude, and sample time-stamp, Xi, Yi, and ti, respectively. Approximation A, which is a subset of P, the polyline of the original trajectory, is the name given to the simplified version of a trajectory \cite{van2017extensive}. Moreover, the original trajectory's p1 and pn must be included in Approximation A. 
In the thesis we will focus on a spatial metrics in order to compare the trajectory and its simple version.  

\subsection{Hausdorff Distance}

  
One simple similarity measure for trajectories is the Hausdorff distance. It is a very
general similarity measure that can be used for any two point sets. If we have two
sets of points P and Q, such as two trajectories if we treat them as polygonal curves
and disregard the timestamps, the directed Hausdorff distance from P to Q is equal
to the Euclidean distance between the point in P that is furthest from any point in
Q, and the point in Q that is closest to that point \cite{kerkhof2022algorithmic}. Written formally, we get:

$$\overrightarrow{\mathrm{H}}(P, Q)=\max _{p \in P} \min _{q \in Q} \| p-q \| $$

The undirected Hausdorff distance, also just called the Hausdorff distance, is then
the maximum of the directed distances in both directions.

$$\mathrm{H}(P, Q)=\max \{\overrightarrow{\mathrm{H}}(P, Q), \overrightarrow{\mathrm{H}}(Q, P)\} $$


\subsection{Frechet Distance}

 
Another trajectory similarity measure that we use often is the Fréchet distance. It
is based on the principle that similar polygonal curves should not just be close in
space, but there should also exist some parametrization of the curves such that if
we traverse both simultaneously we should remain close at all times \cite{kerkhof2022algorithmic}. Closeness
here is defined as having small Euclidean distance. The trajectories are treated as
polygonal curves and the timestamps are not taken into account. For polygonal
curves/trajectories P and Q, with P and Q probes respectively, the Fréchet distance
is defined as:

$$\mathrm{F}(P, Q)=\inf _{(\sigma, \theta)} \max _{j:[0,1]}\|P(\sigma(j))-Q(\theta(j))\| $$

where  $ \sigma $   and $ \theta  $  are continuous non-decreasing functions from [0, 1] to the real intervals [1, n] and [1,m], respectively.
This is often explained with the following analogy: Suppose someone is walking
their dog. P is the path the owner takes, and Q is the path of the dog. The owner
and dog can change their speed at will, but they cannot go backwards on the path.
The Fréchet distance is then the shortest possible length the dog’s leash can have
for this walk to be possible.
The weak Fréchet distance is similar, but the constraint is dropped that  $ \sigma $    and $ \theta $  
are non-decreasing. Going back to the man-walking-dog analogy this means the
man and dog can freely move backwards and forwards over their path if this results
in a shorter leash being needed. The weak Fréchet distance between curves thus
gives a lower bound for the (strong) Fréchet distance.
The discrete Fréchet distance is a variant where  $ \sigma $    and $ \theta $   are discrete functions
from {0,..., k} to {1,..., n} and {1,...,m} with the property that 
$0 \leq \sigma (i+1)-\sigma    (i) \leq 1$
and $0 \leq  \theta   (i + 1)-\theta   (i) \leq 1$. This has been explained as someone walking a pet frog,
where instead of walking along edges of the polygonal curve, the owner and frog can
only hop from vertex to vertex. The discrete Fréchet distance between two curves
gives an upper bound on their (continuous) Fréchet distance.
As you might expect, computing the exact Fréchet distance between two curves
is not completely straightforward. Alt and Godau \cite{AltGodau} described an approach for
computing this distance. They de $ \sigma $   ne an algorithm for solving a decision variant of
the problem of computing the distance. This algorithm can answer if the Fréchet
distance between two curves is at most some value $ \delta $ . Then they use a technique
called parametric search to  $ \sigma $   nd the minimum value of $ \delta $ .
Their decision algorithm works by constructing what is called a free space diagram
for a value for $ \delta $ .
It consists of a grid of $   (n - 1)  \times  (m - 1)$ cells, where each cell corresponds to a pair
of line segments, one from P and one fromQ. Each column of cells corresponds to an
edge of P and each row corresponds to an edge of Q. For example, the point (2.6, 3.5)
in a free space diagram corresponds to the probe gotten by linearly interpolating
between P’s second and third probes with a $ \lambda $  of 0.6, and the probe gotten by linearly
interpolating between Q’s third and fourth probes with a $ \lambda $  of 0.5. Each cell is divided
into free space and forbidden space. If the Euclidean distance between the curves at a
point is less than or equal to $ \delta $ , the point is in the free space. If the distance is greater
than $ \delta $ , the point lies in the forbidden space. See Figure \ref{fig:fsd}.


\begin{figure}[!h]
\centering
\includegraphics[width=1.0\linewidth]{figures/Figure.jpg}
\caption{Enter Two polygonal curves P (in blue) and Q (in red), and their free space
diagram for the chosen value of  $ \delta $. The free space is shown in white and the forbidden
space is shown in gray. Each cell of the FSD corresponds to the combination of
one edge of P and one edge of Q. (s, t) is a free point in the diagram, lying on a
reachable path in the free space. One green spot is marked in both the FSD and on
the associated spots on P and Q. A x- and y- monotone path contained in the free
space from (1, 1) to (8, 10), shown in green, corresponds to parametrizations of P and
Q realizing a Fréchet distance of at most  $ \delta $ \cite{kerkhof2022algorithmic}.}
\label{fig:fsd}
\end{figure}
\vspace{1cm}
Now, an x- and y-monotone path from the point (1, 1) to (n,m) entirely through
the free space corresponds to parametrizations of P and Q such that the distance
between the two is at most $ \delta $  at any time, i.e. the Fréchet distance is at most $ \delta $ .
For additional details we refer to the paper by Alt and Godau \cite{AltGodau}.

\subsection{Dynamic Time Warping Distance}
%TO DO% Describe the dynamic time warping distance similarities

\section{Line Simplification Algorithm}
Line simplification deals with the simplification of arbitrary lines, which can be straight or curved. The goal is to reduce the complexity of lines while still ensuring that the simplified representation captures the main features of the original lines. This algorithm address the problem of Line simplification this problem who is relevant for GPU computing and Spatial Data processing

\subsection{Douglas-Peucker Algorithm}
The Douglas-Peucker algorithm  \cite{douglas1973algorithms} \cite{hershberger1994n} takes a polyline
P a sequence of points $\{p_{1},..., p_{n}\}$ , and a user defined
allowed spatial error, $\varepsilon$ > 0. The algorithm builds an
approximation polyline $P'$ , initially consisting of $p_{1}$ and
$p_{n}$. It continues adding the point pi out of the original
polyline that has the largest shortest-euclideandistance
to $P'$ until that distance is smaller than $\varepsilon$ as
demonstrated in Figure \ref{fig:dgpk}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/dgpk.png}
    \caption{Illustration of how the Douglas-Peucker algorithm
iteratively simplifies a line. The allowed spatial
error  $\varepsilon$  is depicted with green circles \cite{van2017extensive}. }
    \label{fig:dgpk}
\end{figure}

\subsection{Visvalingam-Whyatt Algorithm}

The Visvalingam-Whyatt algorithm \cite{doi:10.1179/000870493786962263} uses the concept
of ‘effective area’, which is the area of the triangle
formed by a point and its two neighbors. The algorithm
takes a polyline P a sequence of points $\{p_{1},..., p_{n}\}$, and
a user defined allowed spatial displacement error, $\varepsilon$ > 0.
For every set of three consecutive points $\{p_{i-1},p_{i},p_{i+1}\}$
a triangle is formed with its surface being the ‘effective area’. Iteratively point $p_{i}$ is dropped that results in
the least areal displacement to form an approximation
as illustrated in Figure \ref{fig:visv}. This process halts when the
‘effective area’ is larger than $\varepsilon$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/visv.png}
    \caption{Illustration of how the Visvalingam-Whyatt
algorithm iteratively simplifies the line \cite{van2017extensive}.}
    \label{fig:visv}
\end{figure}
\newpage

\subsection{Imai-Iri}
The basis of the Imai-Iri algorithm \cite{IMAI198631} lies in the construction
of an unweighted directed acyclic graph G.
This graph is constructed by connecting all combinations
of two points that would create an allowed shortcut.
A breadth-first search is done on this graph to
compute the shortest path connecting the first and last
point, resulting in the approximation.
This algorithm takes a polyline P a sequence of
points $\{p_{1},..., p_{n}\}$, and a user defined allowed spatial
error, $\varepsilon $ > 0. For each combination of two points ($p_{i}$ and $p_{j}$) it checks if a line between them intersects all
circles with radius $\varepsilon $ that center on the points that lie{\tiny {\tiny }}
between them $\{p_{x} > p_{i},p_{x} < p_{j}\}$. When this is the
case, the line $p_{i}p_{j}$ is an allowed shortcut and is added
to the graph G, see Figure \ref{fig:imai}. After all allowed shortcuts
are added to graph G, breadth-first search is done
to find the shortest path through the graph from $p_{1}$ to $p_{n}$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/imaiiri.png}
    \caption{Illustration of how the Imai-Iri algorithm generates
shortcuts. Green lines are allowed shortcuts, red
lines are not allowed \cite{van2017extensive}.}
    \label{fig:imai}
\end{figure}

\newpage

\section{Streaming Process Algorithm}

Traditional data-management systems software is built on the concept of persistent
data sets that are stored reliably in stable storage and queried/updated several
times throughout their lifetime. For several emerging application domains, however,
data arrives and needs to be processed on a continuous (24×7) basis, without
the benefit of several passes over a static, persistent data image \cite{garofalakis2016data}. 

It is therefore important to have algorithm to process those stream data.

\subsection{Data-Stream}

Perhaps the most basic synopsis of a data stream is a sample of elements from the
stream. A key benefit of such a sample is its flexibility: the sample can serve as input
to a wide variety of analytical procedures and can be reduced further to provide
many additional data synopses \cite{garofalakis2016data}. If, in particular, the sample is collected using random
sampling techniques, then the sample can form a basis for statistical inference
about the contents of the stream. Data-stream sampling problems require the application of many ideas and techniques
from traditional database sampling, but also need significant new innovations,
especially to handle queries over infinite-length streams. Indeed, the unbounded
nature of streaming data represents a major departure from the traditional
setting. We draw an important distinction
between a stationary window, whose endpoints are specified times or specified
positions in the stream sequence, and a sliding window whose endpoints move forward
as time progresses. Examples of the latter type of window include “the most
recent n elements in the stream” and “elements that have arrived within the past
hour.” Sampling from a finite stream is a special case of sampling from a stationary
window in which the window boundaries correspond to the first and last stream
elements. When dealing with a stationary window, many traditional tools and techniques
for database sampling can be directly brought to bear. In general, sampling
from a sliding window is a much harder problem than sampling from a stationary
window: in the former case, elements must be removed from the sample as they
expire, and maintaining a sample of adequate size can be difficult.We also consider
“generalized” windows in which the stream consists of a sequence of transactions
that insert and delete items into the window; a sliding window corresponds to the
special case in which items are deleted in the same order that they are inserted.

In the context of this thesis we will investigate the technology of POSTGIS on data stream and adapt the algorithm based on the techniques used by POSTGIS for data stream. 

\subsection{Stream Processing}

Data streams can be generated in various scenarios,
including a network of sensor nodes, a stock market or a
network monitoring system and so on \cite{namiot2015big}. It exist multiple techniques that could be done on 
a data stream such as continuous queries,clustering,classification,frequent items mining,outlier and anomaly detection.A stream processing solution has to solve different
challenges \cite{Wähner_2014} :
\begin{itemize}
\item  Processing massive amounts of streaming events (filter, aggregate, rule, automate, predict, act, monitor, alert)
\item  Real-time responsiveness to changing market conditions
\item  Performance and scalability as data volumes increase in size and complexity
\item  Rapid integration with existing infrastructure and data sources: Input (e.g. market data, user inputs, files, history data from a DWH) and output (e.g. trades, email alerts, dashboards, automated reactions)
\item  Fast time-to-market for application development and deployment due to quickly changing landscape and requirements
\item  Developer productivity throughout all stages of the application development lifecycle by offering good tool support and agile development
\item  Analytics: Live data discovery and monitoring, continuous query processing, automated alerts and reactions
\item  Community (component / connector exchange, education / discussion, training / certification)
\item  End-user|ad-hoc continuous query access
\item  Alerting
\item  Push-based visualization
\end{itemize}

\subsection{Technologies}
In this section, we discuss some technological solutions for
data streams processing.
Apache Storm is a distributed real-time computation system
for processing large volumes of high-velocity data \cite{jain2014learning}. Is a
distributed real-time computation system for processing fast,
large streams of data. Storm is an architecture based on
master-workers paradigm. So a Storm cluster mainly
consists of a master and worker nodes, with coordination
done by Zookeeper.

Spark Streaming \cite{spark} is an extension of the core Spark API
\cite{shoro2015big} that enables scalable, high-throughput, fault-tolerant
stream processing of live data streams. Data can be ingested
from many sources like Kafka, Flume, Twitter, ZeroMQ,
Kinesis, or TCP sockets, and can be processed using
complex algorithms expressed with high-level functions like
map, reduce, join and window, see Figure \ref{fig:streams}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{figures/streaming-arch.png}
    \caption{Spark Streaming model representation as we can see it can receive multiple data sources and output to multiple data sources \cite{spark}.}
    \label{fig:streams}
\end{figure}



Finally, processed data can be pushed out to files systems,
databases, and live dashboards. In fact, you can apply
Spark’s machine learning and graph processing algorithms
on data streams, see Figure \ref{fig:procs}).

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{figures/streaming-flow.png}
    \caption{Spark Processing \cite{spark}.}
    \label{fig:procs}
\end{figure}



Apache Samza \cite{samza} is a distributed stream processing
framework. It uses Apache Kafka for messaging, and
Apache Hadoop YARN to provide fault tolerance, processor
isolation, security, and resource management see Figure \ref{fig:samza}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/samza-arch4.png}
	\caption{Apache Samza \cite{samza}.}
	\label{fig:samza}
\end{figure}



Apache Flume \cite{flume} is a distributed, reliable, and available
service for efficiently collecting, aggregating, and moving
large amounts of log data. It has a simple and flexible
architecture based on streaming data flows. It is robust and
fault tolerant with tunable reliability mechanisms and many
failovers and recovery mechanisms. It uses a simple
extensible data model that supports online analytic
applications see Figure \ref{fig:flume}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/DevGuide_image00.png}
	\caption{Apache Flume \cite{flume}.}
	\label{fig:flume}
\end{figure}


Apache Kafka itself is often used as a kernel for data stream
architecture. Originally, Apache Kafka is publish-subscribe
messaging rethought as a distributed commit log \cite{ApacheKA}.
Apache Kafka is a distributed system designed for streams.
It is built to be fault-tolerant, high-throughput, horizontally
scalable, and allows geographically distributing data streams
and processing. See Figure \ref{fig:kafka} illustrates stream-centric
architecture in Confluent blog  \cite{Kafka}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/data-flow-768x584.png}
	\caption{Stream-centric architecture on Apache Kafka \cite{Kafka}.}
	\label{fig:kafka}
\end{figure}

Amazon Kinesis \cite{kinesis} is a fully managed, cloud-based
service for real-time data processing over large, distributed
data streams. Amazon Kinesis can continuously capture and
store terabytes of data per hour from hundreds of thousands
of sources such as website clickstreams, financial
transactions, social media feeds, IT logs, and locationtracking
events.



IBM InfoSphere Streams \cite{ballard2014ibm,ibmdoc} is an advanced analytic
platform that enables the development and execution
of applications that process information in data streams.
InfoSphere Streams enables continuous and fast analysis of massive volumes
of moving data to help improve the speed of business insight and decision making see Figure \ref{fig:ibm}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/model.jpg}
	\caption{IBM InfoSphere Streams \cite{ibmdoc}.}
	\label{fig:ibm}
\end{figure}


\subsection{Streaming for Line Simplification}
In this section we will discuss the current state of line simplification algorithm/problem in the context of stream processing.
The main problematic of this thesis is related to this problem because of that this subject will have a more in depth analysis.

As mentionned in "Streaming algorithms for line simplification" \cite{abam2007streaming}, suppose we are tracking one, or maybe many,
moving objects. Each object is equipped with a device that
is continuously transmitting its position. Thus we are re-
ceiving a stream of data points that describes the path along
which the object moves. The goal is to maintain this path
for each object. We are interested in the scenario where we
are tracking the objects over a very long period of time, as
happens for instance when studying the migratory patterns
of animals. In this situation it may be undesirable or even
impossible to store the complete stream of data points. In-
stead we have to maintain an approximation of the input
path. This leads us to the following problem: we are receiving a (possibly infinite) stream $p0,p1,p2,...$ of points in
the plane, and we wish to maintain a simplification (of the
part of the path seen so far) that is as close to the original
path as possible, while using not more than a given (fixed)
amount of available storage.

\subsubsection{Formalization}
In this subsection we will formalize the problem of line simplification in a streaming mode based on the article \cite{abam2007streaming}.
To be able to state the problem we wish to solve and the results we obtain more precisely, we first introduce some terminology and definitions.
Let $p_0, p_1, \ldots$ be the given stream of input points. We use $P(n)$ to denote the path defined by the points
$p_0, p_1, \ldots, p_n$ - that is, the path connecting those points in order - and for any two points $p, q$ on the path we use
$P(p, q)$ to denote the subpath from $p$ to $q$. For two vertices $p_i, p_j$ we use $P(i, j)$ as a shorthand for $P\left(p_i, p_j\right)$.
A segment $p_i p_j$ with $i<j$ is called a link or sometimes a shortcut. Thus $P(n)$ consists of the links $p_{i-1} p_i$ for $0<i \leqslant n$.
We assume a function error is given that assigns a non-negative error to each link $p_i p_j$.
A $\ell$-simplification of $P(n)$ is a polygonal path $Q:=q_0, q_1, \ldots, q_k, q_{k+1}$ where
$k \leqslant \ell$ and $q_0=p_0$ and $q_{k+1}=p_n$, and $q_1, \ldots, q_k$ is a subsequence of
$p_1, \ldots, p_{n-1}$. The error of a simplification $Q$ for a given function error, denoted
$\operatorname{error}(Q)$, is defined as the maximum error of any of its links.

\subsubsection{Evaluation}
Now consider an algorithm $\mathcal{A}:=\mathcal{A}(\ell)$ that maintains an $\ell$-simplification for the input stream $p_0, p_1, \ldots$,
for some given $\ell$. Let $Q_{\mathcal{A}}(n)$ denote the simplification that $\mathcal{A}$ produces for the path $P(n)$. Let $\operatorname{Opt}(\ell)$
denote an optimal off-line algorithm that produces an $\ell$-simplification. Thus $\operatorname{error}\left(Q_{O p t(\ell)}(n)\right)$ is the minimum possible
error of any $\ell-$ simplification of $P(n)$. We define the quality of $\mathcal{A}$ using the competitive ratio, as is standard for on-line algorithms.
We also allow resource augmentation. More precisely, we allow $\mathcal{A}$ to use a $2k$-simplification, but we compare the error of this simplification
to $Q_{O p t(k)}(n)$. (This is similar to Agarwal et al. \cite{agarwal2005near} who compare the quality of their solution to the min- $k$ problem for a given maximum error $\delta$
to the optimal value for maximum error $\delta / 2$.) Thus we define the competitive ratio of an algorithm $\mathcal{A}(2 k)$ as
$$
\text { competitive ratio of } \mathcal{A}(2 k):=\max _{n \geqslant 0} \frac{\operatorname{error}\left(Q_{\mathcal{A}(2 k)}(n)\right)}{\operatorname{error}\left(Q_{O p t(k)}(n)\right)},
$$
where $\frac{\operatorname{error}\left(Q_{\mathcal{A}(2 k)}(n)\right)}{\operatorname{error}\left(Q_{O p t(k)}(n)\right)}$ is defined as
1 if $\operatorname{error}\left(Q_{\mathcal{A}(2 k)}(n)\right)=$ $\operatorname{error}\left(Q_{O p t(k)}(n)\right)=0$.
We say that an algorithm is $c$ competitive if its competitive ratio is at most $c$.

\subsubsection{Algorithm}
We will discuss the existing algorithm for the line simplification in a streaming model. In the article \cite{abam2007streaming} they propose
the following algorithm. Our algorithm is quite simple. Suppose we have already handled the points $p_0, \ldots, p_n$.
(We assume $n>\ell+1$; until that moment we can simply use all points and have zero error.) Let $Q:=q_0, q_1, \ldots, q_{\ell}, q_{\ell+1}$
be the current simplification. Our algorithm will maintain a priority queue $\mathcal{Q}$ that stores the points $q_i$ with $1 \leqslant i \leqslant \ell$,
where the priority of a point is the error (as computed by the oracle) of the link $q_{i-1} q_{i+1}$. In other words, the priority of $q_i$ is
(an approximation of) the error that is incurred when $q_i$ is removed from the simplification. Now the next point $p_{n+1}$ is handled as follows:

\begin{enumerate}
	\item  Set $q_{\ell+2}:=p_{n+1}$, thus obtaining an $(\ell+1)$-simplification of $P(n+1)$.
	\item  Compute $\operatorname{error}^*\left(q_{\ell} q_{\ell+2}\right)$ and insert $q_{\ell+1}$ into $\mathcal{Q}$ with this error as priority.
	\item  Extract the point $q_s$ with minimum priority from $\mathcal{Q}$; remove $q_s$ from the simplification.
	\item  Update the priorities of $q_{s-1}$ and $q_{s+1}$ in $\mathcal{Q}$.
	
\end{enumerate}

As we can see the following algorithm is using the error function in order to correct the result in real-time. In the context of this thesis we will investigate a solution
that is based on the existing algorithm douglas-peucker to see if it is possible to maintain a simplified version of the trip using this algorithm.




\subsection{Moving Object Database}
In order to store the position of our tracking objects it is important to
search on moving object database as data output for this thesis. Moving objects are objects that change their value or location
with time. These can be vehicles, persons, animals, aircarft, the air
temperature of a city, the fuel price in a certain gas station, etc.
The ubiquity of tracking devices and IoT technologies has resulted
in collecting massive amounts of data that describe the temporal
evolution of such objects and values. This creates opportunities
to build applications on this data, which in turn calls for building \cite{zimanyi2019mobilitydb}.

As mentioned above the need for a specific database in order to have an efficient data storage.
In the context of this thesis we will focus on mobilityDB which is a Moving Object database that extends
PostgreSQL

\subsubsection{PostgreSQL}

PostgreSQL is an object-relational database management system (ORDBMS) based on POSTGRES, Version 4.2,
developed at the University of California at Berkeley Computer Science Department \cite{postgresql}. POSTGRES pioneered many
concepts that only became available in some commercial database systems much later.

PostgreSQL is an open-source descendant of this original Berkeley code. It supports a large part of the SQL standard
and offers many modern features:

\begin{itemize}
	
	\item complex queries
	\item foreign keys
	\item triggers
	\item updatable views
	\item transactional integrity
	\item multiversion concurrency control
\end{itemize}

Also, PostgreSQL can be extended by the user in many ways, for example by adding new

\begin{itemize}
	\item data types
	\item functions
	\item operators
	\item aggregate functions
	\item index methods
	\item procedural languages
\end{itemize}
And because of the liberal license, PostgreSQL can be used, modified, and distributed by anyone free of charge for any purpose, be it private, commercial, or academic.


\subsubsection{PostGIS}
PostGIS represents a potent open-source instrument facilitating the creation of resilient spatial databases. Serving as the geographic extension of the PostgreSQL database management system, PostGIS enables the incorporation of geographic objects within data tables \cite{marquez2015postgis}. These geographic objects are specialized data types designed for the storage of geographic positions or sets thereof, integrated seamlessly into lines or polygons. In essence, PostGIS emerges as a formidable tool, empowering users to manage intricate geographical data proficiently and to visually interrogate such data when employed in conjunction with graphical tools, exemplified by QGIS.

\subsubsection{MobilityDB}
MobilityDB uses the abstract data type approach of MOD implementations
\cite{guting2000foundation}. In an extensible relational database system this
amounts to adding user-defined types that can be used as attribute
types inside relations. MobilityDB defines in PostgreSQL and PostGIS
the temporal types: \texttt{tgeompoint}, \texttt{tgeogpoint}, \texttt{tfloat}, \texttt{tint},
\texttt{ttext}, and \texttt{tbool}. These types encode functions from the time
domain to their corresponding base type domains. MobilityDB extends the existential
spatial database by adding a new dimension. It becomes more dynamics and respond to the need 
of processing moving object data. 


\chapter{Design}
\section{SQUISH-E}
In this section we introduce the SQUISH-E algorithm, present a pseudo-code and analyze its complexity. \\

The SQUISH-E algorithm compresses a trajectory \(T\) by utilizing two parameters, \(\lambda\) and \(\mu\), to strategically minimize the Synchronized Euclidean Distance (SED) error while achieving a specified compression ratio (\(\lambda\)). It operates by compressing \(T\) until further compression would result in an increase in SED error above \(\mu\), with a notable case being SQUISH-E(\(\lambda\)) where setting \(\mu\) to 0 focuses on minimizing SED error to achieve the compression ratio of \(\lambda\). In contrast, SQUISH-E(\(\mu\)) highlights a scenario where \(\lambda\) is set to 1, aiming to maximize the compression ratio without exceeding the SED error threshold defined by \(\mu\). A pivotal aspect of SQUISH-E is its use of a priority queue \(Q\), in which the priority of each point is determined by an upper bound on the SED error that could be introduced by its removal. This mechanism allows SQUISH-E to efficiently identify and remove the point with the lowest priority, i.e., the point whose removal would result in the least increase in SED error, in \(O(\log |Q|)\) time. This process effectively controls the growth of SED error, ensuring the algorithm's efficiency in compressing trajectory data while maintaining the integrity of the spatial information \cite{muckell2014compression}. In order to have a fully online algorithm SQUISH-E(\(\mu\)) is not taken into account and we only take into account SQUISH-E(\(\lambda\)). The error function is the same but in our implementation it could be a nice idea to compare different error function to find the best ones for some situation.

\subsection{Algorithm Pseudo-Code}
In this section we will present the pseudo-code of SQUISH-E algorithm used in our work. \\

\begin{algorithm}[H]
	\DontPrintSemicolon
	\KwIn{trajectory $T$, lower bound $\lambda$ on compression ratio as a percentage value}
	\KwOut{trajectory $T'$}
	$\beta \gets 4$ \tcp*{the initial capacity of $Q$ is 4}
	\ForEach{point $P_i \in T$}{
		\If{$ i * lambda \geq \beta$}{
			$\beta \gets \beta + 1$ \tcp*{increase the capacity of $Q$}
		}
		set\_priority($P_i, \infty, Q$) \tcp*{enqueue $P_i$ with the priority of $P_i$ being $\infty$}
		$\pi[P_i] \gets 0$\;
		\If{$i > 1$}{
			$succ[P_{i-1}] \gets P_i$ \tcp*{register $P_i$ as $P_{i-1}$'s closest successor}
			$pred[P_i] \gets P_{i-1}$ \tcp*{register $P_{i-1}$ as $P_i$'s closest predecessor}
			adjust\_priority($P_{i-1}, Q, pred, succ, \pi$) \tcp*{Algorithm 3}
		}
		\If{$|Q| = \beta$}{
			reduce($Q, pred, succ, \pi$) \tcp*{Algorithm 2}
		}
	}
	\Return{trajectory $T'$ comprising the points in $Q$ in the order reflected in the $succ$ map}
	\caption{SQUISH-E($T, \lambda$)}
	\label{alg:squish_e}
\end{algorithm}

In the algorithm \ref{alg:squish_e} we can analyze the time complexity. L'algorithme fonctionne de maniére itérationnelle en recevant un par un chaque point ce qui est pratique pour notre cas streaming. La complexité est donc au minima $O(n * max(O(loop) ))$ . La variable lambda as également été modifié. Nous allons ensuite présenter les algorithmes suivants adjust priority \ref{alg:adjust_priority} et reduce \ref{alg:reduce}. 


\begin{algorithm}[H]
	\DontPrintSemicolon
	
	\KwIn{priority queue $Q$, maps $\text{pred}, \text{succ}$ and $\pi$ (refer to Table 2)}
	$P_j \gets \text{remove\_min}(Q); \quad$ // the lowest priority point is removed from $Q$\;
	
	$\pi[succ[P_j]] \gets \text{max}(priority(P_j) , \pi[succ[P_j]]); \quad$  // update neighbor priority\;
	$\pi[pred[P_j]] \gets \text{max}(priority(P_j) , \pi[pred[P_j]]); \quad$ // update neighbor priority\;
	
	
	$succ[\text{pred}[P_j]] \gets succ[P_j]; \quad$ // update neighbor's neighbor of $P_j$ \;
	$\text{pred}[succ[P_j]] \gets \text{pred}[P_j]; \quad$ // update neighbor's neighbor of $P_j$ \;
	
	
	$\text{adjust\_priority}(\text{pred}[P_j], \text{pred}, \text{succ}, \pi); \quad$ // Algorithm 3\;
	$\text{adjust\_priority}(succ[P_j], \text{pred}, \text{succ}, \pi); \quad$ // Algorithm 3\;
	remove the entry for $P_j$ from $\text{pred}, \text{succ},$ and $\pi; \quad$ // garbage collection\;
	
	\caption{reduce($Q, \text{pred}, \text{succ}, \pi$)}
	\label{alg:reduce}
\end{algorithm}



\begin{algorithm}[H]
	\DontPrintSemicolon
	
	\KwIn{point $P_j$, priority queue $Q$, maps $\text{pred}, \text{succ}$ and $\pi$ (refer to Table 2)}
	\If{$\text{pred}[P_j] \neq \text{null}$ and $\text{succ}[P_j] \neq \text{null}$}{
		$p \gets \pi[\text{SED}(P_j,\text{pred}[P_j], \text{succ}[P_j])];$\;
		$\text{set\_priority}(P_j, p, Q);$\;
	}
	
	\caption{adjust\_priority($P_j, Q, \text{pred}, \text{succ}, \pi$)}
	\label{alg:adjust_priority}
\end{algorithm}


Nous pouvons constater que pour obtenir la complexité optimale de cette algorithme il est impératif que l'opération la plus couteuse de cette boucle se fait en $O(log(n))$. La suite seras un focus sur les variables et leur méthodes afin de determiner les élèments d'implémentation requis. 

\subsection{Variables}


\begin{table}[h!]
	\centering
	\label{tab:variables}
	\begin{tabular}{ll}
		\hline
		\textbf{Variable} & \textbf{Description} \\
		\hline
		$Q$             & Priority queue \\
		$p_{\pi}$       & Point with the lowest priority \\
		$\text{pred}$   & Predecessor map \\
		$\text{succ}$   & Successor map \\
		$\pi$           & Priority map \\
		$P_i$           & A point in the priority queue \\
		$\text{SED}$    & Some distance function \\
		\hline
	\end{tabular}
    \caption{List of Variables Used in Algorithms}
\end{table}

\subsubsection{Map}

As mentioned before we have 3 map two that has as pair key-value point-point and another with point-priority . 


\paragraph{Method}
The method that is necessary to implement for this map is the following

\begin{itemize}
	\item set(key,value,map)
	\item remove(key,map)
	\item get(key,map) -> value
\end{itemize}

\subsubsection{PriorityQueue}

Nous avons également une autre structure étant une priority queue mais ayant des méthodes assez différentes. La priority queue stocke des élèments étant des paires point-priority et posséde une structure trié afin de pouvoir retirer le minimum. Cette structure pose un véritable challenge du à la nature et aux méthodes qui seront définis ci dessous. 

\paragraph{Method}
Les méthodes nécessaires afin d'implémenter la priorityqueue sont les suivants :

\begin{itemize}
	\item set\_priority(key,value,Q) this function can be expressed as the two following methods
	\begin{itemize}
		\item remove(key,Q) or replace(key,Q) // depends on implementation
		\item push(key,value,Q)
	\end{itemize}
	\item remove\_min(Q)
\end{itemize}

La fonction set étant plus proche de la caractéristique d'un array que d'une priorityqueue cela pose un challenge si les ressources existantes ne permettent pas d'implémenter ces méthodes en $O(log(n))$ au maximum. 



\chapter{Implementation}
Ce chapitre se concentre sur l'implémentation des élèments précisé au chapitre précedent.

\section{SQUISH-E Implementation}


\subsection{Algorithm 1}

\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={SQUISH-E}, % Légende du listing
	label=lst:squish_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	
	void
	iteration_simplification_sqe(void *p_i , void *p_j ,
	size_t *beta,const double lambda,int i,
	Dict *succ,Dict *pred,
	PDict  *p,struct PriorityQueue *Q,
	bool syncdist,interpType interp ,bool hasz ,uint32_t minpts)
	{
		if( i * lambda >= *beta)
		{
			*beta += 1;
		}
		set_priority_queue(p_i,INF,Q);
		set_priority_dict(p_i,0,p);
		if(i >= 1)
		{
			set_point_dict(p_i,p_j,pred);
			set_point_dict(p_j,p_i,succ);
			adjust_priority(p_j,Q,pred,succ,p, syncdist, interp , hasz );
		}
		size_t size = size_queue(Q);
		if(size - *beta == 0 ){
			reduce(Q,pred,succ,p,syncdist, interp , hasz );
		}
	}
\end{lstlisting}
\vspace{1cm}
This algorithm follow the pseudo code of SQUISH-E it called the set function of the priority queue and the set function of the point map and priority map that will be describe in the section about implementation of variable. Then the reduce function and adjust priority is called in order to reduce if the size meet the treshold and the adjustment for each point that has a predecessor and a successor in order to know the error that it will produce by removing the corresponding point. 

\subsection{Algorithm 2}
\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={reduce}, % Légende du listing
	label=lst:reduce_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	
	void
	reduce(struct PriorityQueue *Q,Dict *pred,Dict *succ,PDict  *p,
	bool syncdist,interpType interp ,bool hasz )
	{
		struct PriorityQueueElem *entry = remove_min(Q);
		size_t size_before = size_queue(Q);
		
		void * p_j = entry->point;
		double priority = entry->priority;
		
		void * p_i = get_point_dict(p_j,pred);
		void * p_k = get_point_dict(p_j,succ);
		
		double pr_i = get_priority_dict(p_i,p); if(priority > pr_i){ pr_i = priority; }
		double pr_k = get_priority_dict(p_k,p); if(priority > pr_k){ pr_k = priority; }
		
		set_priority_dict(p_k, pr_k ,p);
		set_priority_dict(p_i, pr_i ,p);
		
		set_point_dict(p_i,p_k,succ);
		set_point_dict(p_k,p_i,pred);
		set_point_dict(p_k,p_i,pred);
		
		adjust_priority(p_k ,Q,pred,succ,p,syncdist, interp ,hasz );
		adjust_priority(p_i ,Q,pred,succ,p,syncdist, interp ,hasz );
		
		
		//Delete pointer
		free(entry);
		destroy_elem_PriorityDict(p_j,p);
		destroy_elem_PointDict(p_j,succ);
		destroy_elem_PointDict(p_j,pred);
		
	}
	
\end{lstlisting}

This algorithm describe the reduction method as mentioned above remove the lowest priority points from the queue and it update the neighbors. Because we use C langage we free memory of the useless data. 

\subsection{Algorithm 3}

\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={adjust\_priority}, % Légende du listing
	label=lst:adjust_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	
void
adjust_priority(void *p_i,struct PriorityQueue *Q, Dict *pred,Dict *succ,PDict  *p,
bool syncdist,interpType interp ,bool hasz )
{
	
	void * p_h = get_point_dict(p_i,pred);
	void * p_k = get_point_dict(p_i,succ);
	if( p_h != NULL &&  p_k != NULL )
	{
		if(syncdist)
		{
			double priority = get_priority_dict(p_i,p) + SED(p_h,p_i,p_k, interp , hasz );
			set_priority_queue(p_i,priority,Q);
		}
	}
}	

\end{lstlisting}

\section{Variables Implementation}
Dans cette section nous allons discuter de la manière dont les variables ont été implémenté et de leur efficacité ensuite discuter des performances et de la complexité atteinte. 

\subsection{Map}
Cette section se concentre sur l'implémentation du map

\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={Map C implementation}, % Légende du listing
	label=lst:map_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	
	#include <search.h>
	
	
	typedef struct PriorityDict
	{
		void * key;
		double priority;
	} PriorityDict;
	
	
	typedef struct PointDict
	{
		void *key;
		void *value;
	} PointDict;
	
	
	typedef void* Dict;
	typedef void* PDict;
	
\end{lstlisting}
\vspace{1cm}
This structure is a Map that link a point to a corresponding value like priority or another point. We just need to define a struct with two values the key and the value. The typedef void* is using for the GNU Library to avoid using void** and add more clarity to the code. 

\subsubsection{Method}

\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={Point Map Methods}, % Légende du listing
	label=lst:pmap_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	
	
	int compar(const void *l, const void *r)
	{
		const PointDict *lm = l;
		const PointDict *lr = r;
		return lm->key - lr->key;
	}
	
	void pd_free(void *l)
	{
		free(l);
	}
	
	
	void *
	get_point_dict(void *p_i,Dict *dict)
	{
		PointDict find;
		find.key = p_i;
		void * result = tfind(&find, dict, compar);
		if(result){
			result = (*(PointDict**)result)->value;
		}
		return result;
	}
	
	
	void
	set_point_dict(void * p_i,void * p_j,Dict *dict)
	{
		PointDict *find = malloc(sizeof(PointDict));
		find->key = p_i;
		find->value = p_j;
		void * result = tfind(find, dict, compar);
		if(result){
			(*(PointDict**)result)->value = p_j;
		}
		else{
			tsearch(find, dict, compar); /* insert */
		}
	}
	
	
	void destroy_PointDict(void *dict)
	{
		tdestroy(dict,pd_free);
	}
	
	
	void destroy_elem_PointDict(void * p_i,Dict *dict)
	{
		PointDict find;
		find.key = p_i;
		void * r = tfind(&find, dict, compar);
		if(r)
		{
			PointDict * to_free = (*(PointDict**)r);
			tdelete(&find,dict,compar);
			free(to_free);
		}
	}
	
\end{lstlisting}

\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={Priority Map method}, % Légende du listing
	label=lst:prmap_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	
	
	int comparp(const void *l, const void *r)
	{
		const PriorityDict *lm = l;
		const PriorityDict *lr = r;
		return lm->key - lr->key;
	}
	
	void p_free(void *l)
	{
		free(l);
	}
	
	
	double
	get_priority_dict(void *p_i,PDict *dict)
	{
		double result = -1;
		PriorityDict find;
		find.key = p_i;
		void * res = tfind(&find, dict, comparp);
		if(res){
			result = (*(PriorityDict**)res)->priority;
		}
		return result;
	}
	
	
	void
	set_priority_dict(void * p_i,double priority,PDict *dict)
	{
		PriorityDict *find = malloc(sizeof(PriorityDict));
		find->key = p_i;
		find->priority = priority;
		void * result = tfind(find, dict, comparp);
		if(result){
			(*(PriorityDict**)result)->priority = priority;
		}
		else{
			tsearch(find, dict, comparp); /* insert */
		}
	}
	
	
	void destroy_PriorityDict(PDict dict)
	{
		tdestroy(dict,p_free);
	}
	
	
	void destroy_elem_PriorityDict(void * p_i,PDict *dict)
	{
		PriorityDict find;
		find.key = p_i;
		void * r = tfind(&find, dict, comparp);
		if(r)
		{
			PriorityDict * to_free = (*(PriorityDict**)r);
			tdelete(&find,dict,comparp);
			free(to_free);
		}
	}
	
\end{lstlisting}
\vspace{1cm}
Here we use function of GNU C Library that implement a binary tree. It has the property to have for each operation a maximal complexity of $O(log(n))$ The two mapping use the same implementation in reality it would be a good idea to resolve this duplication case using a template or thing that can resolve it but the C language limit the capability. 

\subsubsection{Discussion}
As mentioned in the previous sub section the map use GNU C Library binary tree structure to perform search and set. Those algorithm have a complexity of $O(log(n))$ as mentioned before and it has the property to be have a dynamic allocation of memory. 

\subsection{PriorityQueue}

\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={PriorityQueue}, % Légende du listing
	label=lst:prqueue_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	
	typedef struct PriorityQueueElem
	{
		void * point;
		double priority;
		int index; //reference his own index
	} PriorityQueueElem;
	
	typedef void* IDict;
	
	typedef struct PriorityQueue
	{
		PriorityQueueElem **arr;
		IDict dict;
		size_t size;
		size_t capacity;
	} PriorityQueue;
	
\end{lstlisting}
\vspace{1cm}
This structure implements the function of a priority queue using the structure of a min heap. It has as value PriorityQueueElem which have the point and the corresponding priority and a reference to this index in the heap. The main structure is using an array of pointer of PriorityQueueElem as long as an index map, a int that represent the number of points in the queue and lastly the numbers points allocated. 

\subsubsection{Method}

\begin{lstlisting}[language=C, % Spécifie le langage du code
	caption={PriorityQueue Method}, % Légende du listing
	label=lst:prqueueMethod_c, % Étiquette pour référencer le listing
	numbers=left, 
	numberstyle=\tiny\color{gray}, 
	stepnumber=1, 
	frame=single,
	breaklines=true, 
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	showstringspaces=false 
	]
	


struct PriorityQueue *
create_PriorityQueue(int capacity){
	// Allocating memory to heap h
	PriorityQueue* h = (PriorityQueue*)malloc(sizeof(PriorityQueue));
	
	// Checking if memory is allocated to h or not
	if (h == NULL) {
		//printf("Memory error");
		return NULL;
	}
	// set the values to size and capacity
	h->size = 0;
	h->capacity = capacity;
	
	// Allocating memory to array
	h->arr = (PriorityQueueElem**)malloc(capacity * sizeof(PriorityQueueElem*));
	
	// Checking if memory is allocated to h or not
	if (h->arr == NULL) {
		//printf("Memory error");
		return NULL;
	}
	return h;
}


size_t size_queue(const PriorityQueue *Q)
{
	return Q->size;
}

void minHeapify(PriorityQueue* Q, int index)
{
	int left = index * 2 + 1;
	int right = index * 2 + 2;
	int min = index;
	
	// Checking whether our left or child element
	// is at right index or not to avoid index error
	if (left >= Q->size || left < 0)
	left = -1;
	if (right >= Q->size || right < 0)
	right = -1;
	
	// store left or right element in min if
	// any of these is smaller that its parent
	if (left != -1 && Q->arr[left]->priority < Q->arr[index]->priority)
	min = left;
	if (right != -1 && Q->arr[right]->priority < Q->arr[index]->priority)
	min = right;
	
	// Swapping the nodes
	if (min != index) {
		PriorityQueueElem *temp = Q->arr[min];
		Q->arr[min] = Q->arr[index];
		Q->arr[index] = temp;
		
		Q->arr[min]->index = min;
		Q->arr[index]->index = index;
		
		// recursively calling for their child elements
		// to maintain min heap
		minHeapify(Q, min);
	}
}


int compar_index(const void *l, const void *r)
{
	const PriorityQueueElem *lm = l;
	const PriorityQueueElem *lr = r;
	return lm->point - lr->point;
}

PriorityQueueElem *remove_min(PriorityQueue *Q)
{
	PriorityQueueElem *result = NULL;
	if (Q->size != 0) {
		result = Q->arr[0];
		// Replace the deleted node with the last node
		Q->arr[0] = Q->arr[Q->size - 1];
		Q->arr[Q->size - 1] = NULL;
		if(Q->arr[0]){
			Q->arr[0]->index = 0;
		}
		
		// Decrement the size of heap
		Q->size--;
		
		tdelete(result,&Q->dict,compar_index);
		// Call minheapify_top_down for 0th index
		// to maintain the heap property
		minHeapify(Q, 0);
		
	}
	return result;
}


PriorityQueueElem *get_elem(void *p_i,IDict *dict)
{
	PriorityQueueElem find;
	find.point = p_i;
	PriorityQueueElem * result = tfind(&find, dict, compar_index);
	if(result){
		result = (*(PriorityQueueElem**)result);
	}
	return result;
	
}




struct PriorityQueueElem *
replace_elem(void *p_i,double priority ,PriorityQueue *Q)
{
	PriorityQueueElem *elem = get_elem(p_i,&Q->dict);
	if(elem ){
		//elog(NOTICE,"FOUND ELEM FOR SUR \n Index :  %i / SIZE : %i ",elem->index ,Q->size);
		if(Q->size > elem->index && elem->index != -1){
			//elog(NOTICE,"REPLACE ELEM FOR SUR");
			Q->arr[elem->index]->priority = priority;
			insertHelper(Q, elem->index);
			minHeapify(Q, elem->index);
		}
	}
	return elem;
}

void insertHelper(PriorityQueue* Q, int index)
{
	// Store parent of element at index
	// in parent variable
	int parent = (index - 1) / 2;
	
	if (index > 0 && index < Q->size && parent >= 0 && parent < Q->size) {
		if( Q->arr[parent]->priority > Q->arr[index]->priority){
			// Swapping when child is smaller
			// than parent element
			PriorityQueueElem *temp = Q->arr[parent];
			Q->arr[parent] = Q->arr[index];
			Q->arr[index] = temp;
			
			Q->arr[parent]->index = parent;
			Q->arr[index]->index = index;
			
			// Recursively calling insertHelper
			insertHelper(Q, parent);
		}
	}
	
}


void
push(PriorityQueueElem * insert,PriorityQueue *Q)
{
	Q->size++;
	if(Q->size > Q->capacity){
		Q->arr = realloc(Q->arr, Q->size * sizeof(PriorityQueueElem*));
	}
	Q->arr[Q->size-1] = insert;
	Q->arr[Q->size-1]->index = Q->size-1;
	insertHelper(Q, Q->size-1);
}


void
set_priority_queue(void *p_i,double priority ,PriorityQueue *Q)
{
	struct PriorityQueueElem * insert = replace_elem(p_i,priority,Q);
	if(insert == NULL){
		//elog(NOTICE,"INSERT ELEM FOR SUR");
		insert = malloc(sizeof(struct PriorityQueueElem));
		insert->point = p_i;
		insert->priority = priority;
		insert->index = -1;
		tsearch(insert, &Q->dict, compar_index); /* insert */
	}
	if(insert->index == -1){
		push(insert,Q);
	}
}

void i_free(void *l)
{
	free(l);
}

void destroy_Queue(struct PriorityQueue *Q)
{
	free(Q->arr);
	tdestroy(Q->dict,i_free);
	free(Q);
}
	
\end{lstlisting}

From other implementation this one has the property to have at most a complexity of $O(log(n))$ the choice of a min heap to implement the priorityqueue fullfill the complexity limitation that we see in the design phase. The min heap is structure has a structure of a tree like we see in Figure \ref{fig:min_heap} and has the property that each tree and sub tree has the property that the parent nodes is always lesser than the child nodes. The min heap implementation is inspired from this website \ref{GfG_2023}.



\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/tree.png}
	\caption{Min Heap Structure}
	\label{fig:min_heap}
\end{figure}


\subsubsection{Discussion}

The idea here is to use the min heap structure to obtain in a complexity of maximum $O(log(n))$  the point with the lowest priority. In the side we use a index map in order to replace point in the heap in an efficient way. Due to the fact that the design include the set of an element when it is already in the array. This structure has the advantage to benefit from static memory. The set works using the mapping to see if the point is already present in the map. If it is we replace it in the heap by changing his priority and apply a check with his ancestor then his descendant. If the point is not present we add a value at the end of the heap and check and update their ancestor to satisfy the condition of a min heap. So at the end the two operations needed for the algorithm reach a complexity of $O(log(n))$ 

\section{Postgres Implementation}

In this section we describe how the algorithm is implemented into the mobilityDB sql functions. 
\subsection{SQL Code}

\begin{lstlisting}[
	language=SQL, % Setting the language for SQL
	caption={SQUISHE SQL Code}, % Caption for the listing
	label=lst:squish_sql, % Label for referencing the listing
	basicstyle=\ttfamily\small, % Basic style
	keywordstyle=\color{blue}\ttfamily, % Style for keywords
	stringstyle=\color{red}\ttfamily, % Style for strings
	commentstyle=\color{green}\ttfamily, % Style for comments
	numbers=left, % Line numbers on the left
	numberstyle=\tiny\color{gray}, % Style for line numbers
	stepnumber=1, % Line numbers step
	frame=single,
	breaklines=true, % Automatic line breaking
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}, % Arrow for line breaks
	showstringspaces=false % Don't show special spaces within strings
	]
	CREATE FUNCTION SquishESimplify(tfloat, float, boolean DEFAULT TRUE)
	RETURNS tfloat
	AS 'MODULE_PATHNAME', 'Temporal_simplify_sqe'
	LANGUAGE C IMMUTABLE STRICT PARALLEL SAFE;
	CREATE FUNCTION SquishESimplify(tgeompoint, float, boolean DEFAULT TRUE)
	RETURNS tgeompoint
	AS 'MODULE_PATHNAME', 'Temporal_simplify_sqe'
	LANGUAGE C IMMUTABLE STRICT PARALLEL SAFE;
\end{lstlisting}

\subsection{Example}
This section show some examples of usages of the pgsql function describe in \ref{lst:squish_sql}. This allowed to use the algorithm in an offline settings using request. 
\begin{lstlisting}[
	language=SQL, % Setting the language for SQL
	caption={Example SQL Code}, % Caption for the listing
	label=lst:example_sql, % Label for referencing the listing
	basicstyle=\ttfamily\small, % Basic style
	keywordstyle=\color{blue}\ttfamily, % Style for keywords
	stringstyle=\color{red}\ttfamily, % Style for strings
	commentstyle=\color{green}\ttfamily, % Style for comments
	numbers=left, % Line numbers on the left
	numberstyle=\tiny\color{gray}, % Style for line numbers
	stepnumber=1, % Line numbers step
	frame=single,
	breaklines=true, % Automatic line breaking
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}, % Arrow for line breaks
	showstringspaces=false % Don't show special spaces within strings
	]
	
	SELECT SquishESimplify(tfloat '[1@2000-01-01, 2@2000-01-02, 3@2000-01-04,
	4@2000-01-05]', '1 day');
	-- [1@2000-01-01, 3@2000-01-04]
	
	SELECT asText(SquishESimplify(tgeompoint '[Point(1 1 1)@2000-01-01,
	Point(2 2 2)@2000-01-02, Point(3 3 3)@2000-01-04, Point(5 5 5)@2000-01-05)', 0.5));
	-- [POINT Z (1 1 1)@2000-01-01, POINT Z (3 3 3)@2000-01-04,POINT Z (5 5 5)@2000-01-05)
	
\end{lstlisting}

\subsection{Result}

Here we will talk about the result of the function and see the precision and the performance of the requests. We will also compare it with the C implementation in real time using the same data to compare the offline implementation with the online one. 

\subsection{Performance}
This section will recolt and discuss the performance of the sql function by varying the lambda parameter and the number of points. 
\begin{table}[htbp]
	\centering
	\label{tab:execution_time}
	\begin{tabular}{@{}lccccc@{}}
		\toprule
		Number of Points & \multicolumn{5}{c}{Lambda} \\
		\cmidrule{2-6}
		& 1         & 0.75       & 0.5        & 0.25       & 0.01       \\
		\midrule
		100              & 00.001137 & 00.001965 & 00.002775 & 00.003328 & 00.004529 \\
		1000             & 00.01498  & 00.014966 & 00.022183 & 00.024588 & 00.023035 \\
		10000            & 00.164196 & 00.215692 & 00.214011 & 00.247379 & 00.218447 \\
		100000           & 01.77365  & 02.171435 & 02.844324 & 02.617976 & 02.437037 \\
		1000000          & 04.618455 & 05.371317 & 06.461961 & 06.021071 & 04.510934 \\
		\bottomrule
	\end{tabular}
    \caption{Average Execution Time by Number of Points and Lambda}
\end{table}

In order to get those results we benchmark the request 10 times for each requests and retrieve the average of those executions. As standalone results it has no real meaning outside but we can notice that the speed of the algorithm increase around $0.5$ and decrease outside those values of lambda. We can also state that maybe there is another value as maximum execution time between $0.75$ and $0.25$ . We can state that this is maybe because the number of instruction that mix setting in a priority queue and the instructions of reduction is around $0.5$. Because when lambda is equal to 1 there is no reduction and only setting in the maximal size of a priority queue and adjust priority operation and when lambda go towards $0$ there is a lot of reduction operation and setting in the minimal size of a priority queue. That is an explanation that could befit those data. When the size of the input is multiplying by $10$ the execution time is around $2$ times longer. 

\subsubsection{Comparison with C}
In order to have meaning to those result we will compare it with the C implementation using the real time approach and to see between the offline and the online approach the difference that happen.

\begin{table}[htbp]
	\centering
	\label{tab:execution_time_c}
	\begin{tabular}{@{}lccccc@{}}
		\toprule
		Number of Points & \multicolumn{5}{c}{Lambda} \\
		\cmidrule{2-6}
		& 1         & 0.75       & 0.5        & 0.25       & 0.01       \\
		\midrule
		100              & 00.000094 & 00.000092 & 00.00009 & 00.000098 & 00.000096 \\
		1000             & 00.00081  & 00.000822 & 00.000943 & 00.000876 & 00.000788 \\
		10000            & 00.008889 & 00.008852 & 00.008663 & 00.008263 & 00.00818 \\
		100000           & 00.090018  & 00.088485 & 00.089533 & 00.089064 & 00.088728 \\
		1000000          & 00.892172 & 00.864116 & 00.870661 & 00.937186 & 00.920517 \\
		\bottomrule
	\end{tabular}
	\caption{Average Execution Time (C) by Number of Points and Lambda}
\end{table}

As we can see, executing C code is much faster than executing sql queries, which makes execution and simplification using steam processing techniques in C both viable and possible. The speed is such that even for 1 million points the average execution time is less than 1 second. 

\subsection{Precision}
This section will focus on the precision of the simplification using the same parameter as before in order to have an idea of the quality of the simplification in a function of lambda and types of path. We will discuss on trajectory based on AIS dataset. As stated in \cite{abam2007streaming}, there is different type of path as convex and concave. As a reminder this work focus on the simplification of lines and do not take into account land or see restriction during the simplification process. In this section we will analyze different trajectory defines them and see with different lambda and metrics evaluation the quality of the simplification. At the end we will run in a big datasets and see the results as a graph and discuss them. 

\subsubsection{Trajectory 1}
 
 \begin{figure}[!h]
 	\centering
 	\includegraphics[width=0.5\linewidth]{figures/Stats/traj_1.jpg}
 	\caption{Trajectory 1}
 	\label{fig:traj_1}
 \end{figure}

This trajectory is composed of 20865 points and begin at 1am the 8/1/2021 with a duration of 1 day. This trajectory have convex and concave path in order to analyze the effect of the simplification in those specific moment. The precision of the trajectory will be computed based on the current metrics proposed in the state of the art such as frechet distance and hausdorff distances. We will not discuss the precision of lambda 1 because it does not reduce any points from the original trajectory. 



\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{figures/Stats/squish_1.jpg}
	\caption{Trajectory 1 - SQUISH-E}
	\label{fig:traj_1_squish}
\end{figure}

The path is looks really similar and is close to the original path in \ref{fig:traj_1_squish}. In order to see the differences we can zoom and move the slider in order to see the moving objects in relation with the current time. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{figures/Stats/squish_1_zoom.jpg}
	\caption{Trajectory 1 - ZOOM }
	\label{fig:traj_1_sqzoom}
\end{figure}

In this we can see the path being simplified where there is curves and having less precision as lambda decreases.  

\begin{table}[htbp]
	\centering
	\label{tab:precision_metrics}
	\begin{tabular}{@{}lcccc@{}}
		\toprule
		 & \multicolumn{4}{c}{Lambda} \\
		\cmidrule{2-5}
		& 0.75       & 0.5        & 0.25       & 0.01       \\
		\midrule
		Number of points           & 15646 & 10431 & 5215 & 208 \\
		Frechet Distance              & 0.012 & 0.012 & 0.012 & 0.066 \\
		Hausdorff Distance             & 0.012 & 0.012 & 0.012 & 0.066 \\
		DynTimeWarp Distance            & 1.045 & 3.028 & 8.190 & 281.413\\
		\bottomrule
	\end{tabular}
	\caption{Precision metrics per Lambda for Trajectory 1 }
\end{table}

This table gives a view of the precision of squish-e on the first trajectory. The data shows that it is consistent and precise and that errors is increasing when lambda decreases. Dynamic Time Warping Distance gives also an overview of the loss of precision when we remove more points. 

\subsubsection{Comparison With C}
This section will outline the differences between the SQL and C execution since the SQL represents an offline execution and C represents the online the differences here is to underline the possible loss of accuracy in the process. In order to keep this part concise a table with all variables above will be given and choose the differences of the distances between the offline and the online trajectories. 




\chapter{Comparison}
This chapter focus on the comparison of our implementation with existing other implementation in MobilityDB.

\section{Douglas Peucker}
\subsection{Performance}
\subsection{Similarity}

\section{MinDist}
\subsection{Performance}
\subsection{Similarity}

\chapter{Conclusion}

\newpage
\printbibliography
%\bibliography{reference}
%\bibliographystyle{abbrv}


\end{document}
